{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa518513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2262/2262 - 7s - loss: 0.7462 - val_loss: 0.7274 - 7s/epoch - 3ms/step\n",
      "Epoch 2/5\n",
      "2262/2262 - 5s - loss: 0.6982 - val_loss: 0.7162 - 5s/epoch - 2ms/step\n",
      "Epoch 3/5\n",
      "2262/2262 - 5s - loss: 0.6915 - val_loss: 0.7126 - 5s/epoch - 2ms/step\n",
      "Epoch 4/5\n",
      "2262/2262 - 5s - loss: 0.6887 - val_loss: 0.7106 - 5s/epoch - 2ms/step\n",
      "Epoch 5/5\n",
      "2262/2262 - 5s - loss: 0.6869 - val_loss: 0.7089 - 5s/epoch - 2ms/step\n",
      "2262/2262 [==============================] - 3s 1ms/step\n",
      "566/566 [==============================] - 1s 1ms/step\n",
      "Most important features (by name): Index(['St1_Torque_Knee_Z', 'St1_Angle_Pelvis_Z', 'St1_Torque_Knee_Y',\n",
      "       'St1_Angle_Knee_X', 'St1_GRF_Y', 'St1_Torque_Ankle_X',\n",
      "       'St1_Torque_Pelvis_Z', 'St1_Angle_Pelvis_Y', 'St1_Angle_Pelvis_X',\n",
      "       'St1_Angle_Hip_Z', 'St1_Angle_Hip_Y', 'St1_Angle_Knee_Z',\n",
      "       'St1_Angle_Hip_X', 'St1_Angle_Ankle_Y', 'St1_Angle_Ankle_X',\n",
      "       'St1_GRF_Z', 'Time', 'St1_Torque_Pelvis_X', 'St1_Torque_Hip_X',\n",
      "       'St1_Torque_Hip_Z', 'St1_Angle_Knee_Y', 'St1_GRF_X', 'St1_Torque_Hip_Y',\n",
      "       'St1_Torque_Pelvis_Y', 'St1_Angle_Ankle_Z', 'St1_Torque_Knee_X',\n",
      "       'Emg_raw_St1_BF', 'Emg_raw_St1_VL', 'St1_Torque_Ankle_Z',\n",
      "       'St1_Torque_Ankle_Y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load your data from a CSV file\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "data = pd.read_csv('4_Raw_data_csv.csv')\n",
    "\n",
    "# Select the output feature (label)\n",
    "y = data['St1_Angle_Hip_X']\n",
    "\n",
    "# Select all other features except 'St1_Angle_Hip_X' and 'Time'\n",
    "X = data.drop(columns=['St1_Angle_Hip_X', 'Time'])\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "# Define the number of input features\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Set the number of dimensions for the latent space (compressed representation)\n",
    "latent_dim = 10  # Adjust according to how much you want to reduce the features\n",
    "\n",
    "# Define the encoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "latent = Dense(latent_dim, activation='relu')(encoded)\n",
    "\n",
    "# Define the decoder\n",
    "decoded = Dense(32, activation='relu')(latent)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "output_layer = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# Combine the encoder and decoder into an autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=5,\n",
    "                batch_size=32,\n",
    "                validation_data=(X_test, X_test),\n",
    "                verbose=2)\n",
    "# Create a separate model for the encoder to extract the latent features\n",
    "encoder = Model(inputs=input_layer, outputs=latent)\n",
    "\n",
    "# Use the encoder to transform the data\n",
    "X_train_encoded = encoder.predict(X_train)\n",
    "X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "# Extract the weights of the first layer in the encoder\n",
    "encoder_weights = encoder.layers[1].get_weights()[0]\n",
    "\n",
    "# Analyze which original features contribute most to each latent feature\n",
    "feature_importance = np.mean(np.abs(encoder_weights), axis=1)\n",
    "\n",
    "# Sort features by importance\n",
    "important_features_indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "# Map the indices to feature names\n",
    "important_features = data.columns[important_features_indices]\n",
    "\n",
    "print(\"Most important features (by name):\", important_features)\n",
    "# Select the most important features from the original data\n",
    "# You can choose a threshold for the number of features you want to keep\n",
    "# Using .iloc for index-based selection\n",
    "selected_features = X.iloc[:, important_features_indices[:10]]  # Keep top 10 features, adjust as needed\n",
    "\n",
    "\n",
    "# Convert selected features back to a DataFrame if you need to save or further process\n",
    "selected_features_df = pd.DataFrame(selected_features, columns=important_features[:10])\n",
    "\n",
    "# Save the selected features to a new CSV file\n",
    "selected_features_df.to_csv('selected_featuresSt1_Angle_Hip_X.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
